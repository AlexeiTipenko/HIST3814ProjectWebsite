{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\n\n\nThe purpose of this project was to create a compelling story for the data found in the 14th Canadian General Hospital War Diaries. This collection can be found \nhere\n, or it can be viewed on my \nGitHub page\n. After looking at the files, it was difficult to see how I could use digital history to get any kind of valuable information from these photographs. For one, it was difficult to understand the writing. There were several spelling and grammatical mistakes contained within the \"Summary of Events and Information\" section. The format of the locations and dates were very inconsistent, especially with the different authors involved. However, after looking at these photographs for some time, I started to see certain patterns that may be useful.\n\n\nIn the next few sections, I will explain my process of understanding the documents, transcribing their contents and cleaning the data using various tools and techniques. Finally, I will discuss what I have learned from the data by presenting some visualizations.\n\n\nAll the contents of this project can be found \nhere\n on my GitHub page. If you are interested in extending the project, feel free to fork the repository and make your own additions.", 
            "title": "1) Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "The purpose of this project was to create a compelling story for the data found in the 14th Canadian General Hospital War Diaries. This collection can be found  here , or it can be viewed on my  GitHub page . After looking at the files, it was difficult to see how I could use digital history to get any kind of valuable information from these photographs. For one, it was difficult to understand the writing. There were several spelling and grammatical mistakes contained within the \"Summary of Events and Information\" section. The format of the locations and dates were very inconsistent, especially with the different authors involved. However, after looking at these photographs for some time, I started to see certain patterns that may be useful.  In the next few sections, I will explain my process of understanding the documents, transcribing their contents and cleaning the data using various tools and techniques. Finally, I will discuss what I have learned from the data by presenting some visualizations.  All the contents of this project can be found  here  on my GitHub page. If you are interested in extending the project, feel free to fork the repository and make your own additions.", 
            "title": "Introduction"
        }, 
        {
            "location": "/understanding/", 
            "text": "Understanding the War Diaries\n\n\nBefore diving deep into the project, I was curious to see how successful Tesseract to perform OCR (object character recognition) on the 80 photographs in the collection. The results were quite disappointing. Here is an example of a text file converted using Tesseract:\n\n\n\n\nIf you look through all the \ntext files\n, you can see that all of them are of the same quality. Unfortunately, Tesseract and other OCR tools are simply not good enough to accurately transcribe the these photographs. The only solution here is to transcribe them all by hand.\n\n\nTwo of my classmates, \nLauren Rollit\n and \nAmanda Ngan\n worked with me to transcribe all 80 of the original photographs into separate text files. Lauren was responsible for transcribing the first 58 documents. I transcribed the next 12, and Amanda transcribed the remaining 10. The transcribed documents can be found \nhere\n.\n\n\nBefore working on cleaning the files, I spend a lot of time looking through the transcribed files to understand what insights I can get from them. One thing was immediately clear from the start, I could not throw all the files into one and start cleaning the whole file. The first thing to note is that all the files fell into one of three broad categories: a normal diary entry, an outlier (different format) and a cover page that contained the volume numbers. I also noticed that about a third of the documents were not from the 14th hospital at all. These documents were from the 10th Canadian hospital. I noticed this by looking at the location documented either on the top of bottom of every document. The documents from the 10th hospital also had a different format. They were written by a different author who documented locations more frequently and was in the habit of documenting every event with only one date.\n\n\nThis was the key difference between the documents from the two hospitals. The author from the 14th hospital basically never wrote down the location of the events, and almost always had two dates documented for every event. I made the assumption that the first date was the documented date of the given event. I deduced this from the fact that this date often corresponded to multiple events and the second date was also usually a few days earlier. Because the structure of the documents from the hospitals varied a good deal, I decided to separate the ones from the 10th hospital and focus on working with only the ones from the 14th hospital.\n\n\nBased on the specific documents I was working with, I found some consistent patterns that I can use to help me understand the contents of the diaries on a macro level. These included the documented date, event description, event date and the count of the various workers at specific points in time. The data on the count was also separated into a different file to keep everything organized.\n\n\nHere is a list of steps I took to organize the data:\n\n\n1) Combine all transcribed text documents into one file using the following command:\n    cat * \n 1-merge_initial.txt\n\n2) Trim the redundant and unnecessary data from the initial text file and add \"DOC#:XXXX\" before each section to know which original document I'm dealing with. (2-merge_trim.txt)\n\n3) Separate the trimmed text file into 3: 3-main_pages.txt, 4-outliers_pages.txt and 5-volume_pages.txt.\n\n The main mages file consists of the bulk of the data that I was working with. The other two were used as references. They can be useful for future analysis.", 
            "title": "2) Understanding the War Diaries"
        }, 
        {
            "location": "/understanding/#understanding-the-war-diaries", 
            "text": "Before diving deep into the project, I was curious to see how successful Tesseract to perform OCR (object character recognition) on the 80 photographs in the collection. The results were quite disappointing. Here is an example of a text file converted using Tesseract:   If you look through all the  text files , you can see that all of them are of the same quality. Unfortunately, Tesseract and other OCR tools are simply not good enough to accurately transcribe the these photographs. The only solution here is to transcribe them all by hand.  Two of my classmates,  Lauren Rollit  and  Amanda Ngan  worked with me to transcribe all 80 of the original photographs into separate text files. Lauren was responsible for transcribing the first 58 documents. I transcribed the next 12, and Amanda transcribed the remaining 10. The transcribed documents can be found  here .  Before working on cleaning the files, I spend a lot of time looking through the transcribed files to understand what insights I can get from them. One thing was immediately clear from the start, I could not throw all the files into one and start cleaning the whole file. The first thing to note is that all the files fell into one of three broad categories: a normal diary entry, an outlier (different format) and a cover page that contained the volume numbers. I also noticed that about a third of the documents were not from the 14th hospital at all. These documents were from the 10th Canadian hospital. I noticed this by looking at the location documented either on the top of bottom of every document. The documents from the 10th hospital also had a different format. They were written by a different author who documented locations more frequently and was in the habit of documenting every event with only one date.  This was the key difference between the documents from the two hospitals. The author from the 14th hospital basically never wrote down the location of the events, and almost always had two dates documented for every event. I made the assumption that the first date was the documented date of the given event. I deduced this from the fact that this date often corresponded to multiple events and the second date was also usually a few days earlier. Because the structure of the documents from the hospitals varied a good deal, I decided to separate the ones from the 10th hospital and focus on working with only the ones from the 14th hospital.  Based on the specific documents I was working with, I found some consistent patterns that I can use to help me understand the contents of the diaries on a macro level. These included the documented date, event description, event date and the count of the various workers at specific points in time. The data on the count was also separated into a different file to keep everything organized.  Here is a list of steps I took to organize the data:  1) Combine all transcribed text documents into one file using the following command:\n    cat *   1-merge_initial.txt\n\n2) Trim the redundant and unnecessary data from the initial text file and add \"DOC#:XXXX\" before each section to know which original document I'm dealing with. (2-merge_trim.txt)\n\n3) Separate the trimmed text file into 3: 3-main_pages.txt, 4-outliers_pages.txt and 5-volume_pages.txt.\n\n The main mages file consists of the bulk of the data that I was working with. The other two were used as references. They can be useful for future analysis.", 
            "title": "Understanding the War Diaries"
        }, 
        {
            "location": "/cleaning/", 
            "text": "Cleaning Data\n\n\nThe cleaning process for the project was a combination of manual editing and the \"Find and Replace\" option found in most text editors. I personally decided to use atom as my text editor. The find and replace functionality within atom is quite powerful and it has regular expression functionality build in as well.\n\n\nAt the beginning of the cleaning process, there were certain issues that could not be fixed using any tool. For example, there was no way for a tool to recognize when an event spanned multiple lines. I also needed to add the documented dates to every single line to ensure consistency. I couldn't find a regular expression that could help me perform these two tasks.\n\n\nOnce there was a general consistency in the text file with every line consisting of 2 dates and an event, I began to actively use regex to do the rest of the cleaning.\n\n\nA common way of using regex is to use sed commands. Here is an example.\n\n\nsed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt\n\n\n\nThis is definitely a valid method, but I found that using the find and replace option inside atom was both easier and more intuitive. All I had to do was enter my regex command into the find window and enter another command to select the groups I want to keep for the replacement in the replace window. Every time I performed a regex operation, I could clearly see if the operation was successful. If it wasn't, I would simply undo the past command.\n\n\nHere is a list of all the commands I used to clean my text file:\n\n\n1)  ([a-zA-Z][\\.][a-zA-Z][\\.])([\\040])(\\w+)\n2)  ([\\040])([a-zA-Z])([\\.])([\\040])(\\w+)\n3)  ([\\040])(\\bC\\.A\\.M\\.C\\.)(Depot)\n4)  (\\b\\d[/]\\d\\d[/]\\d\\d)\n5)  (\\d\\d[/]\\d\\d[/])(\\d\\d)\n6)  (\\d\\d[/]\\d[/])(\\d\\d)\n7)  ([^\\d])(\\d[/]\\d[/])(\\d\\d)\n8)  (,)(\\d\\d[/]\\d\\d[/])(\\d\\d\\d\\d)(\\.)\n9)  (,)(\\d\\d[/]\\d[/])(\\d\\d\\d\\d)(\\.)\n10) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n11) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n12) ([\\040])([a-zA-Z][\\.][^\\s\\.]+)([\\040])\n\n\n\nMost of these regular expressions do very similar things. The idea was to create a series of groups that equate to the exact format of the text I wanted to change and then simply replace them with the groups necessary.\n\n\nFor example, I wanted to find all instances of 'X.X. WORD'. This exact format is almost always mapped to a name, with some exceptions. My goal was to remove the space between the last period and the name for each instance of this pattern. So I created this regular expression:\n\n\n([a-zA-Z][.][a-zA-Z][.])([\\040])(\\w+)\n\n\n\nAll it says is to find a group that has a letter (1), period (1), letter (2), period (2), then have another group following that represents a space, and then a final group that represents a whole word of any length. After specifying this group, all I had to do was replace the 3 groups with the first and the third.\n\n\n$1$2\n\n\n\nAfter executing this find and replace operation, the letters, periods and one word will stay, but the space in between would be deleted. This is exactly what I wanted and I didn't have to manually search through the whole text file to make all these changes. The rest of the regular expressions worked in an almost identical way.\n\n\nSometimes, even regex wasn't necessary. There were many words and sentences that I could find and replace with another version of them. For example '.SEAFORD' could be replaced with '. Seaford', or 'Hsp' could be replaced with \"Hospital\".\n\n\nThe final cleaning process involved using open refine and trifacta to merge certain words and phrases. Open refine did not work when I tried processing the entirety of the event data. There was simply too much data. I was able to make some small changes using the key collision method, however everything froze for the nearest neighbor method. To get more use out of open refine, I extracted all the names mentioned in the overall text file and did separate cleaning on it. This file was much smaller, and I was able to merge several names together. Trifacta is a software application that also allowed me to tweak a few things in my overall csv file. I was able to rename a few sentences that were almost identical to others to create a more accurate distribution of common phrases.\n\n\nThe end result was \nthis file\n. There is no way to get a completely clean file without doing everything by hand, but I would say that the great majority of the data has been transcribed and cleaned properly.", 
            "title": "3) Cleaning Data"
        }, 
        {
            "location": "/cleaning/#cleaning-data", 
            "text": "The cleaning process for the project was a combination of manual editing and the \"Find and Replace\" option found in most text editors. I personally decided to use atom as my text editor. The find and replace functionality within atom is quite powerful and it has regular expression functionality build in as well.  At the beginning of the cleaning process, there were certain issues that could not be fixed using any tool. For example, there was no way for a tool to recognize when an event spanned multiple lines. I also needed to add the documented dates to every single line to ensure consistency. I couldn't find a regular expression that could help me perform these two tasks.  Once there was a general consistency in the text file with every line consisting of 2 dates and an event, I began to actively use regex to do the rest of the cleaning.  A common way of using regex is to use sed commands. Here is an example.  sed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt  This is definitely a valid method, but I found that using the find and replace option inside atom was both easier and more intuitive. All I had to do was enter my regex command into the find window and enter another command to select the groups I want to keep for the replacement in the replace window. Every time I performed a regex operation, I could clearly see if the operation was successful. If it wasn't, I would simply undo the past command.  Here is a list of all the commands I used to clean my text file:  1)  ([a-zA-Z][\\.][a-zA-Z][\\.])([\\040])(\\w+)\n2)  ([\\040])([a-zA-Z])([\\.])([\\040])(\\w+)\n3)  ([\\040])(\\bC\\.A\\.M\\.C\\.)(Depot)\n4)  (\\b\\d[/]\\d\\d[/]\\d\\d)\n5)  (\\d\\d[/]\\d\\d[/])(\\d\\d)\n6)  (\\d\\d[/]\\d[/])(\\d\\d)\n7)  ([^\\d])(\\d[/]\\d[/])(\\d\\d)\n8)  (,)(\\d\\d[/]\\d\\d[/])(\\d\\d\\d\\d)(\\.)\n9)  (,)(\\d\\d[/]\\d[/])(\\d\\d\\d\\d)(\\.)\n10) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n11) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n12) ([\\040])([a-zA-Z][\\.][^\\s\\.]+)([\\040])  Most of these regular expressions do very similar things. The idea was to create a series of groups that equate to the exact format of the text I wanted to change and then simply replace them with the groups necessary.  For example, I wanted to find all instances of 'X.X. WORD'. This exact format is almost always mapped to a name, with some exceptions. My goal was to remove the space between the last period and the name for each instance of this pattern. So I created this regular expression:  ([a-zA-Z][.][a-zA-Z][.])([\\040])(\\w+)  All it says is to find a group that has a letter (1), period (1), letter (2), period (2), then have another group following that represents a space, and then a final group that represents a whole word of any length. After specifying this group, all I had to do was replace the 3 groups with the first and the third.  $1$2  After executing this find and replace operation, the letters, periods and one word will stay, but the space in between would be deleted. This is exactly what I wanted and I didn't have to manually search through the whole text file to make all these changes. The rest of the regular expressions worked in an almost identical way.  Sometimes, even regex wasn't necessary. There were many words and sentences that I could find and replace with another version of them. For example '.SEAFORD' could be replaced with '. Seaford', or 'Hsp' could be replaced with \"Hospital\".  The final cleaning process involved using open refine and trifacta to merge certain words and phrases. Open refine did not work when I tried processing the entirety of the event data. There was simply too much data. I was able to make some small changes using the key collision method, however everything froze for the nearest neighbor method. To get more use out of open refine, I extracted all the names mentioned in the overall text file and did separate cleaning on it. This file was much smaller, and I was able to merge several names together. Trifacta is a software application that also allowed me to tweak a few things in my overall csv file. I was able to rename a few sentences that were almost identical to others to create a more accurate distribution of common phrases.  The end result was  this file . There is no way to get a completely clean file without doing everything by hand, but I would say that the great majority of the data has been transcribed and cleaned properly.", 
            "title": "Cleaning Data"
        }, 
        {
            "location": "/analysis/", 
            "text": "Insights and Visualizations\n\n\nAfter I ended up with a clean csv file containing all the various events and their corresponding dates, I began to create various visualizations to understand what all the data means.\n\n\nThe first aspect of the data I turned to was the dates. I used RStudio, along with the ggplot2, scales and lubridate packages to create a histogram for both the documented dates and the event dates.\n\n\nHere is the graph for the event dates:\n\n\n\n\nHere is the graph for the documented dates:\n\n\n\n\nThe first histogram contains some outliers, however the two graphs seem to follow an almost identical pattern. The only exception is the unusual rise in documented event dates in the summer months of 1918. What this says about the diaries is that most documented dates were reported at nearly the same time as the actual events. This means that the hospital was quite meticulous when it came to their documentation.\n\n\nWe can also see that there is spike in events at the start of 1918 and another one between September and October of the same year. This means that the hospital had greater activity at these times, which could imply an increase in combatant casualties. From these histograms alone, you can infer the activity of Canada's military in the First World War. Since the war started in 1914, perhaps Canada wasn't all that active at its beginning and made significantly greater contributions during the last year of the war. However, this isn't necessarily the case. Perhaps the hospital was only operational in the last 1 or 2 years of the war, or there are parts of the diaries that were simply not recovered. It's important to take the insights from these histograms with a grain of salt, but they do show an interesting pattern that could gain more validity when combined with more data from this time period.\n\n\nThe other aspect of the data that is quite important is the event description. The main tool I used to analyze this data was \nvoyant tools\n. This tool is very powerful and has generated several visualizations that can be useful. I began by looking at the individual words within the data. After transcribing a portion of the documents myself and working with them closely, what I discovered from voyant was no surprise.\n\n\nHere are the most common words used in the event descriptions:\n\n\n\n\nA large amount of events documented in the diaries were simply \"X taken on strength\" or \"Y struck off the strength\". So the most common words would have to be strength, struck, taken, etc. The diaries also frequently mentioned the C.A.M.C Depot. Several other similar events indicate that it is a \"reserve and training depot\". In other words, this is a location where soldiers are strategically placed for future military action. This particular depot was also a military site where some training took place as well. There are also other common words included in the above visualization. These words are common in the military vocabulary, such as nursing (sister), ranks, casualty, captain, regimental and attached.\n\n\nSo how do these common words interact with each other over time? Here's another visualization:\n\n\n\n\nC.A.M.C., depot, strength and struck all follow the same sort of pattern. All 4 loosely follow the same fluctuations. The interesting this is the dip in relative frequency for the 4 words. This implies that an event like \"X struck off the strength from c.a.m.c. depot\" was used more often near the end of the war. In a casualty form, taken or being struck off strength refers to \nmilitary personnel movement\n within a unit. So taking on strength means entering a certain unit and being struck off strength means leaving a unit. The dip in the visualization could mean that the military units in the C.A.M.C. depot were mobilized for military action and sent off. This implies that less military personnel would be leaving their unit because they would be occupied on the battlegrounds. Another interesting thing about this visualization is how the frequency of the word \"taken\" doesn't seem to change that much. This word frequency by itself could indicate the relative stream of new recruits joining new units. There are multiple combinations of graphs and visualizations that can be built directly with voyant. Feel free to play around with more of my visualizations by heading to this \nlink\n.\n\n\nOne final question I had regarding the data was how the different names mentioned in the event description section compared to one another. Here is another visualization of the most popular names in the diaries:\n\n\n\n\nIt is clear from this visualization that the most common names in the event descriptions had the same relative frequency. C.A.Young has been mentioned 8 times and the next 3 common names are trailing close behind with 5 instances. This says something about the nature of the events in the diary. It is already established that many of the events have to do with the movement of military personnel. So it makes sense for names to only show up a few times. A normal troop would probably be taking on strength for a unit, then leaving it for another unit a year or two later. It wouldn't make sense to constantly move personnel around. It could be the case that the most common names were mistakenly transferred between units a few too many times. Or maybe these people had importance in relation to the hospital and the depot. If you are interested in exploring name specific data through various visualizations, you can find that \nhere\n.\n\n\nHere is another quick and general visualization that I made with trifacta:\n\n\n\n\nThese visualizations have provided some interesting insights on the 14th General Hospital and the potential state of the Canadian military during World War One. Before starting this project, I assumed that the majority of the insights would involve the hospital directly. I was quite surprised to find out that even basic information like military personnel movement can produce insights on the state of the Canadian military at a given time and important dates during the war itself.\n\n\nIf you would like to see any of the files for this project, please visit the project repository my GitHub \npage\n.", 
            "title": "4) Insights and Visualizations"
        }, 
        {
            "location": "/analysis/#insights-and-visualizations", 
            "text": "After I ended up with a clean csv file containing all the various events and their corresponding dates, I began to create various visualizations to understand what all the data means.  The first aspect of the data I turned to was the dates. I used RStudio, along with the ggplot2, scales and lubridate packages to create a histogram for both the documented dates and the event dates.  Here is the graph for the event dates:   Here is the graph for the documented dates:   The first histogram contains some outliers, however the two graphs seem to follow an almost identical pattern. The only exception is the unusual rise in documented event dates in the summer months of 1918. What this says about the diaries is that most documented dates were reported at nearly the same time as the actual events. This means that the hospital was quite meticulous when it came to their documentation.  We can also see that there is spike in events at the start of 1918 and another one between September and October of the same year. This means that the hospital had greater activity at these times, which could imply an increase in combatant casualties. From these histograms alone, you can infer the activity of Canada's military in the First World War. Since the war started in 1914, perhaps Canada wasn't all that active at its beginning and made significantly greater contributions during the last year of the war. However, this isn't necessarily the case. Perhaps the hospital was only operational in the last 1 or 2 years of the war, or there are parts of the diaries that were simply not recovered. It's important to take the insights from these histograms with a grain of salt, but they do show an interesting pattern that could gain more validity when combined with more data from this time period.  The other aspect of the data that is quite important is the event description. The main tool I used to analyze this data was  voyant tools . This tool is very powerful and has generated several visualizations that can be useful. I began by looking at the individual words within the data. After transcribing a portion of the documents myself and working with them closely, what I discovered from voyant was no surprise.  Here are the most common words used in the event descriptions:   A large amount of events documented in the diaries were simply \"X taken on strength\" or \"Y struck off the strength\". So the most common words would have to be strength, struck, taken, etc. The diaries also frequently mentioned the C.A.M.C Depot. Several other similar events indicate that it is a \"reserve and training depot\". In other words, this is a location where soldiers are strategically placed for future military action. This particular depot was also a military site where some training took place as well. There are also other common words included in the above visualization. These words are common in the military vocabulary, such as nursing (sister), ranks, casualty, captain, regimental and attached.  So how do these common words interact with each other over time? Here's another visualization:   C.A.M.C., depot, strength and struck all follow the same sort of pattern. All 4 loosely follow the same fluctuations. The interesting this is the dip in relative frequency for the 4 words. This implies that an event like \"X struck off the strength from c.a.m.c. depot\" was used more often near the end of the war. In a casualty form, taken or being struck off strength refers to  military personnel movement  within a unit. So taking on strength means entering a certain unit and being struck off strength means leaving a unit. The dip in the visualization could mean that the military units in the C.A.M.C. depot were mobilized for military action and sent off. This implies that less military personnel would be leaving their unit because they would be occupied on the battlegrounds. Another interesting thing about this visualization is how the frequency of the word \"taken\" doesn't seem to change that much. This word frequency by itself could indicate the relative stream of new recruits joining new units. There are multiple combinations of graphs and visualizations that can be built directly with voyant. Feel free to play around with more of my visualizations by heading to this  link .  One final question I had regarding the data was how the different names mentioned in the event description section compared to one another. Here is another visualization of the most popular names in the diaries:   It is clear from this visualization that the most common names in the event descriptions had the same relative frequency. C.A.Young has been mentioned 8 times and the next 3 common names are trailing close behind with 5 instances. This says something about the nature of the events in the diary. It is already established that many of the events have to do with the movement of military personnel. So it makes sense for names to only show up a few times. A normal troop would probably be taking on strength for a unit, then leaving it for another unit a year or two later. It wouldn't make sense to constantly move personnel around. It could be the case that the most common names were mistakenly transferred between units a few too many times. Or maybe these people had importance in relation to the hospital and the depot. If you are interested in exploring name specific data through various visualizations, you can find that  here .  Here is another quick and general visualization that I made with trifacta:   These visualizations have provided some interesting insights on the 14th General Hospital and the potential state of the Canadian military during World War One. Before starting this project, I assumed that the majority of the insights would involve the hospital directly. I was quite surprised to find out that even basic information like military personnel movement can produce insights on the state of the Canadian military at a given time and important dates during the war itself.  If you would like to see any of the files for this project, please visit the project repository my GitHub  page .", 
            "title": "Insights and Visualizations"
        }, 
        {
            "location": "/reflection/", 
            "text": "Course Reflection\n\n\nOver the past 6 weeks, I feel like I've learned a lot from this course. History has always been an interesting class for me, but I never thought that it could be integrated so well with the technological tools that we have today.\n\n\nOne of the main things I learned was the importance of looking at historical information through macro lens. Although thorough analysis of a particular historical document can be beneficial, it is also important to be able to take a step back and look at everything from a distant perspective. By simply looking at one photograph from the war diaries, I understood that there was some troop movement and each event had a date. After analyzing all the data from the 14th hospital diaries specifically, I was able to get insights on events outside of the hospital. One approach to data analysis isn't better than the other, but it is important to keep both in mind to obtain better insights and ask better questions.\n\n\nAnother important lesson I've learned was just how long and tedious the data cleaning process can be. I've always known that it takes time, but I never realized just how much effort needs to go into it. Aside from that, getting truly clean data is an incredibly difficult task to achieve. I've learned this first hand after spending way too much time trying to get everything about my data to line up perfectly. When dealing with large amounts of data, it simply becomes too time consuming to fix every little mistake.\n\n\nAside from that, this course has introduced me to several different tools that will be very helpful for me in the future. I have become very comfortable with using git, and I've been exposed to important statistical tools like R and Python. I even learned how to make a blog, which I never expected to do for a history class.\n\n\nMy greatest strength that I've discovered through this course is that I am a lot more technologically capable than I give myself credit. There's an ideal in the programming community that every developer seems to strive towards. This ideal involves a technological guru that creates flawless software at a very fast pace and works at an ambitious startup. My program is fairly competitive partially because of this, and it can make me feel like I'm not a good enough programmer and problem solver as a result. After taking this course, I've been reminded that all of that is not true. I've had a lot of technical issues during this course, but I was almost always able to fix them because I was interested in the class material and I knew when to take a break.\n\n\nMy greatest weakness was probably procrastination and trying to get things to work perfectly the first time. A lot of the stress involved with this course was getting stuck on something small last minute and not quite being able to fix it. In terms of perfectionism, I would expect things to work right away after running a command or finishing an exercise. In the world of software, that almost never happens. When these expectations are constantly ruined by some unknown error, I can easily burn out. I've been getting better at that by scheduling my work better and forcing myself to get away from the computer.\n\n\nThis has been one of my favorite courses at Carleton to date. It was very refreshing to see how technology can interact with the humanities in such a meaningful way.", 
            "title": "5) Course Reflection"
        }, 
        {
            "location": "/reflection/#course-reflection", 
            "text": "Over the past 6 weeks, I feel like I've learned a lot from this course. History has always been an interesting class for me, but I never thought that it could be integrated so well with the technological tools that we have today.  One of the main things I learned was the importance of looking at historical information through macro lens. Although thorough analysis of a particular historical document can be beneficial, it is also important to be able to take a step back and look at everything from a distant perspective. By simply looking at one photograph from the war diaries, I understood that there was some troop movement and each event had a date. After analyzing all the data from the 14th hospital diaries specifically, I was able to get insights on events outside of the hospital. One approach to data analysis isn't better than the other, but it is important to keep both in mind to obtain better insights and ask better questions.  Another important lesson I've learned was just how long and tedious the data cleaning process can be. I've always known that it takes time, but I never realized just how much effort needs to go into it. Aside from that, getting truly clean data is an incredibly difficult task to achieve. I've learned this first hand after spending way too much time trying to get everything about my data to line up perfectly. When dealing with large amounts of data, it simply becomes too time consuming to fix every little mistake.  Aside from that, this course has introduced me to several different tools that will be very helpful for me in the future. I have become very comfortable with using git, and I've been exposed to important statistical tools like R and Python. I even learned how to make a blog, which I never expected to do for a history class.  My greatest strength that I've discovered through this course is that I am a lot more technologically capable than I give myself credit. There's an ideal in the programming community that every developer seems to strive towards. This ideal involves a technological guru that creates flawless software at a very fast pace and works at an ambitious startup. My program is fairly competitive partially because of this, and it can make me feel like I'm not a good enough programmer and problem solver as a result. After taking this course, I've been reminded that all of that is not true. I've had a lot of technical issues during this course, but I was almost always able to fix them because I was interested in the class material and I knew when to take a break.  My greatest weakness was probably procrastination and trying to get things to work perfectly the first time. A lot of the stress involved with this course was getting stuck on something small last minute and not quite being able to fix it. In terms of perfectionism, I would expect things to work right away after running a command or finishing an exercise. In the world of software, that almost never happens. When these expectations are constantly ruined by some unknown error, I can easily burn out. I've been getting better at that by scheduling my work better and forcing myself to get away from the computer.  This has been one of my favorite courses at Carleton to date. It was very refreshing to see how technology can interact with the humanities in such a meaningful way.", 
            "title": "Course Reflection"
        }
    ]
}