{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThe purpose of this project was to create a compelling story for the data found in the 14th Canadian General Hospital War Diaries. This collection can be found \nhere\n, or it can be viewed on my \nGitHub page\n. After looking at the files, it was difficult to see how I could use digital history to get any kind of valuable information from these photographs. For one, it was difficult to understand the writing. There were several spelling and grammatical mistakes contained within the \"Summary of Events and Information\" section. The format of the locations and dates were very inconsistent, especially with the different authors involved. However, after looking at these photographs for some time, I started to see certain patterns that may be useful.\n\n\nIn the next few sections, I will explain my process of understanding the documents, transcribing their contents and cleaning the data using various tools and techniques. Finally, I will discuss what I have learned from the data by presenting some visualizations.\n\n\nAll the contents of this project can be found \nhere\n on my GitHub page. If you are interested in extending the project, feel free to fork the repository and make your own additions.", 
            "title": "1) Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "The purpose of this project was to create a compelling story for the data found in the 14th Canadian General Hospital War Diaries. This collection can be found  here , or it can be viewed on my  GitHub page . After looking at the files, it was difficult to see how I could use digital history to get any kind of valuable information from these photographs. For one, it was difficult to understand the writing. There were several spelling and grammatical mistakes contained within the \"Summary of Events and Information\" section. The format of the locations and dates were very inconsistent, especially with the different authors involved. However, after looking at these photographs for some time, I started to see certain patterns that may be useful.  In the next few sections, I will explain my process of understanding the documents, transcribing their contents and cleaning the data using various tools and techniques. Finally, I will discuss what I have learned from the data by presenting some visualizations.  All the contents of this project can be found  here  on my GitHub page. If you are interested in extending the project, feel free to fork the repository and make your own additions.", 
            "title": "Introduction"
        }, 
        {
            "location": "/understanding/", 
            "text": "Understanding the War Diaries\n\n\nBefore diving deep into the project, I was curious to see how successful Tesseract to perform OCR (object character recognition) on the 80 photographs in the collection. The results were quite disappointing. Here is an example of a text file converted using Tesseract:\n\n\n\n\nIf you look through all the \ntext files\n, you can see that all of them are of the same quality. Unfortunately, Tesseract and other OCR tools are simply not good enough to accurately transcribe the these photographs. The only solution here is to transcribe them all by hand.\n\n\nTwo of my classmates, Lauren Rollit and Amanda Ngan worked with me to transcribe all 80 of the original photographs into separate text files. Lauren was responsible for transcribing the first 58 documents. I transcribed the next 12, and Amanda transcribed the remaining 10. The transcribed documents can be found \nhere\n.\n\n\nBefore working on cleaning the files, I spend a lot of time looking through the transcribed files to understand what insights I can get from them. One thing was immediately clear from the start, I could not throw all the files into one and start cleaning the whole file. The first thing to note is that all the files fell into one of three broad categories: a normal diary entry, an outlier (different format) and a cover page that contained the volume numbers. I also noticed that about a third of the documents were not from the 14th hospital at all. These documents were from the 10th Canadian hospital. I noticed this by looking at the location documented either on the top of bottom of every document. The documents from the 10th hospital also had a different format. They were written by a different author who documented locations more frequently and was in the habit of documenting every event with only one date.\n\n\nThis was the key difference between the documents from the two hospitals. The author from the 14th hospital basically never wrote down the location of the events, and almost always had two dates documented for every event. I made the assumption that the first date was the documented date of the given event. I deduced this from the fact that this date often corresponded to multiple events and the second date was also usually a few days earlier. Because the structure of the documents from the hospitals varied a good deal, I decided to separate the ones from the 10th hospital and focus on working with only the ones from the 14th hospital.\n\n\nBased on the specific documents I was working with, I found some consistent patterns that I can use to help me understand the contents of the diaries on a macro level. These included the documented date, event description, event date and the count of the various workers at specific points in time. The data on the count was also separated into a different file to keep everything organized.\n\n\nHere is a list of steps I tool to organize the data:\n\n\n1) Combine all transcribed text documents into one file using the following command:\n    cat * \n 1-merge_initial.txt\n\n2) Trim the redundant and unnecessary data from the initial text file and add \"DOC#:XXXX\" before each section to know which original document I'm dealing with. (2-merge_trim.txt)\n\n3) Separate the trimmed text file into 3: 3-main_pages.txt, 4-outliers_pages.txt and 5-volume_pages.txt.\n\n The main mages file consists of the bulk of the data that I was working with. The other two were used as references. They can be useful for future analysis.", 
            "title": "2) Understanding the War Diaries"
        }, 
        {
            "location": "/understanding/#understanding-the-war-diaries", 
            "text": "Before diving deep into the project, I was curious to see how successful Tesseract to perform OCR (object character recognition) on the 80 photographs in the collection. The results were quite disappointing. Here is an example of a text file converted using Tesseract:   If you look through all the  text files , you can see that all of them are of the same quality. Unfortunately, Tesseract and other OCR tools are simply not good enough to accurately transcribe the these photographs. The only solution here is to transcribe them all by hand.  Two of my classmates, Lauren Rollit and Amanda Ngan worked with me to transcribe all 80 of the original photographs into separate text files. Lauren was responsible for transcribing the first 58 documents. I transcribed the next 12, and Amanda transcribed the remaining 10. The transcribed documents can be found  here .  Before working on cleaning the files, I spend a lot of time looking through the transcribed files to understand what insights I can get from them. One thing was immediately clear from the start, I could not throw all the files into one and start cleaning the whole file. The first thing to note is that all the files fell into one of three broad categories: a normal diary entry, an outlier (different format) and a cover page that contained the volume numbers. I also noticed that about a third of the documents were not from the 14th hospital at all. These documents were from the 10th Canadian hospital. I noticed this by looking at the location documented either on the top of bottom of every document. The documents from the 10th hospital also had a different format. They were written by a different author who documented locations more frequently and was in the habit of documenting every event with only one date.  This was the key difference between the documents from the two hospitals. The author from the 14th hospital basically never wrote down the location of the events, and almost always had two dates documented for every event. I made the assumption that the first date was the documented date of the given event. I deduced this from the fact that this date often corresponded to multiple events and the second date was also usually a few days earlier. Because the structure of the documents from the hospitals varied a good deal, I decided to separate the ones from the 10th hospital and focus on working with only the ones from the 14th hospital.  Based on the specific documents I was working with, I found some consistent patterns that I can use to help me understand the contents of the diaries on a macro level. These included the documented date, event description, event date and the count of the various workers at specific points in time. The data on the count was also separated into a different file to keep everything organized.  Here is a list of steps I tool to organize the data:  1) Combine all transcribed text documents into one file using the following command:\n    cat *   1-merge_initial.txt\n\n2) Trim the redundant and unnecessary data from the initial text file and add \"DOC#:XXXX\" before each section to know which original document I'm dealing with. (2-merge_trim.txt)\n\n3) Separate the trimmed text file into 3: 3-main_pages.txt, 4-outliers_pages.txt and 5-volume_pages.txt.\n\n The main mages file consists of the bulk of the data that I was working with. The other two were used as references. They can be useful for future analysis.", 
            "title": "Understanding the War Diaries"
        }, 
        {
            "location": "/cleaning/", 
            "text": "Cleaning Data\n\n\nThe cleaning process for the project was a combination of manual editing and the \"Find and Replace\" option found in most text editors. I personally decided to use atom as my text editor. The find and replace functionality within atom is quite powerful and it has regular expression functionality build in as well.\n\n\nAt the beginning of the cleaning process, there were certain issues that could not be fixed using any tool. For example, there was no way for a tool to recognize when an event spanned multiple lines. I also needed to add the documented dates to every single line to ensure consistency. I couldn't find a regular expression that could help me perform these two tasks.\n\n\nOnce there was a general consistency in the text file with every line consisting of 2 dates and an event, I began to actively use regex to do the rest of the cleaning.\n\n\nA common way of using regex is to use sed commands. Here is an example.\n\n\nsed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt\n\n\n\nThis is definitely a valid method, but I found that using the find and replace option inside atom was both easier and more intuitive. All I had to do was enter my regex command into the find window and enter another command to select the groups I want to keep for the replacement in the replace window. Every time I performed a regex operation, I could clearly see if the operation was successful. If it wasn't, I would simply undo the past command.\n\n\nHere is a list of all the commands I used to clean my text file:\n\n\n1)  ([a-zA-Z][\\.][a-zA-Z][\\.])([\\040])(\\w+)\n2)  ([\\040])([a-zA-Z])([\\.])([\\040])(\\w+)\n3)  ([\\040])(\\bC\\.A\\.M\\.C\\.)(Depot)\n4)  (\\b\\d[/]\\d\\d[/]\\d\\d)\n5)  (\\d\\d[/]\\d\\d[/])(\\d\\d)\n6)  (\\d\\d[/]\\d[/])(\\d\\d)\n7)  ([^\\d])(\\d[/]\\d[/])(\\d\\d)\n8)  (,)(\\d\\d[/]\\d\\d[/])(\\d\\d\\d\\d)(\\.)\n9)  (,)(\\d\\d[/]\\d[/])(\\d\\d\\d\\d)(\\.)\n10) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n11) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n12) ([\\040])([a-zA-Z][\\.][^\\s\\.]+)([\\040])\n\n\n\nMost of these regular expressions do very similar things. The idea was to create a series of groups that equate to the exact format of the text I wanted to change and then simply replace them with the groups necessary.\n\n\nFor example, I wanted to find all instances of 'X.X. WORD'. This exact format is almost always mapped to a name, with some exceptions. My goal was to remove the space between the last period and the name for each instance of this pattern. So I created this regular expression:\n\n\n([a-zA-Z][.][a-zA-Z][.])([\\040])(\\w+)\n\n\n\nAll it says is to find a group that has a letter (1), period (1), letter (2), period (2), then have another group following that represents a space, and then a final group that represents a whole word of any length. After specifying this group, all I had to do was replace the 3 groups with the first and the third.\n\n\n$1$2\n\n\n\nAfter executing this find and replace operation, the letters, periods and one word will stay, but the space in between would be deleted. This is exactly what I wanted and I didn't have to manually search through the whole text file to make all these changes. The rest of the regular expressions worked in an almost identical way.\n\n\nSometimes, even regex wasn't necessary. There were many words and sentences that I could find and replace with another version of them. For example '.SEAFORD' could be replaced with '. Seaford', or 'Hsp' could be replaced with \"Hospital\".\n\n\nThe final cleaning process involved using open refine and trifacta to merge certain words and phrases. Open refine did not work when I tried processing the entirety of the event data. There was simply too much data. I was able to make some small changes using the key collision method, however everything froze for the nearest neighbor method. To get more use out of open refine, I extracted all the names mentioned in the overall text file and did separate cleaning on it. This file was much smaller, and I was able to merge several names together. Trifacta is a software application that also allowed me to tweak a few things in my overall csv file. I was able to rename a few sentences that were almost identical to others to create a more accurate distribution of common phrases.\n\n\nThe end result was \nthis file\n. There is no way to get a completely clean file without doing everything by hand, but I would say that the great majority of the data has been transcribed and cleaned properly.", 
            "title": "3) Cleaning Data"
        }, 
        {
            "location": "/cleaning/#cleaning-data", 
            "text": "The cleaning process for the project was a combination of manual editing and the \"Find and Replace\" option found in most text editors. I personally decided to use atom as my text editor. The find and replace functionality within atom is quite powerful and it has regular expression functionality build in as well.  At the beginning of the cleaning process, there were certain issues that could not be fixed using any tool. For example, there was no way for a tool to recognize when an event spanned multiple lines. I also needed to add the documented dates to every single line to ensure consistency. I couldn't find a regular expression that could help me perform these two tasks.  Once there was a general consistency in the text file with every line consisting of 2 dates and an event, I began to actively use regex to do the rest of the cleaning.  A common way of using regex is to use sed commands. Here is an example.  sed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt  This is definitely a valid method, but I found that using the find and replace option inside atom was both easier and more intuitive. All I had to do was enter my regex command into the find window and enter another command to select the groups I want to keep for the replacement in the replace window. Every time I performed a regex operation, I could clearly see if the operation was successful. If it wasn't, I would simply undo the past command.  Here is a list of all the commands I used to clean my text file:  1)  ([a-zA-Z][\\.][a-zA-Z][\\.])([\\040])(\\w+)\n2)  ([\\040])([a-zA-Z])([\\.])([\\040])(\\w+)\n3)  ([\\040])(\\bC\\.A\\.M\\.C\\.)(Depot)\n4)  (\\b\\d[/]\\d\\d[/]\\d\\d)\n5)  (\\d\\d[/]\\d\\d[/])(\\d\\d)\n6)  (\\d\\d[/]\\d[/])(\\d\\d)\n7)  ([^\\d])(\\d[/]\\d[/])(\\d\\d)\n8)  (,)(\\d\\d[/]\\d\\d[/])(\\d\\d\\d\\d)(\\.)\n9)  (,)(\\d\\d[/]\\d[/])(\\d\\d\\d\\d)(\\.)\n10) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n11) ([\\040])([a-zA-Z][\\.][a-zA-Z][\\.][^\\s\\.]+)([\\040])\n12) ([\\040])([a-zA-Z][\\.][^\\s\\.]+)([\\040])  Most of these regular expressions do very similar things. The idea was to create a series of groups that equate to the exact format of the text I wanted to change and then simply replace them with the groups necessary.  For example, I wanted to find all instances of 'X.X. WORD'. This exact format is almost always mapped to a name, with some exceptions. My goal was to remove the space between the last period and the name for each instance of this pattern. So I created this regular expression:  ([a-zA-Z][.][a-zA-Z][.])([\\040])(\\w+)  All it says is to find a group that has a letter (1), period (1), letter (2), period (2), then have another group following that represents a space, and then a final group that represents a whole word of any length. After specifying this group, all I had to do was replace the 3 groups with the first and the third.  $1$2  After executing this find and replace operation, the letters, periods and one word will stay, but the space in between would be deleted. This is exactly what I wanted and I didn't have to manually search through the whole text file to make all these changes. The rest of the regular expressions worked in an almost identical way.  Sometimes, even regex wasn't necessary. There were many words and sentences that I could find and replace with another version of them. For example '.SEAFORD' could be replaced with '. Seaford', or 'Hsp' could be replaced with \"Hospital\".  The final cleaning process involved using open refine and trifacta to merge certain words and phrases. Open refine did not work when I tried processing the entirety of the event data. There was simply too much data. I was able to make some small changes using the key collision method, however everything froze for the nearest neighbor method. To get more use out of open refine, I extracted all the names mentioned in the overall text file and did separate cleaning on it. This file was much smaller, and I was able to merge several names together. Trifacta is a software application that also allowed me to tweak a few things in my overall csv file. I was able to rename a few sentences that were almost identical to others to create a more accurate distribution of common phrases.  The end result was  this file . There is no way to get a completely clean file without doing everything by hand, but I would say that the great majority of the data has been transcribed and cleaned properly.", 
            "title": "Cleaning Data"
        }, 
        {
            "location": "/analysis/", 
            "text": "Insights and Visualizations", 
            "title": "4) Insights and Visualizations"
        }, 
        {
            "location": "/analysis/#insights-and-visualizations", 
            "text": "", 
            "title": "Insights and Visualizations"
        }, 
        {
            "location": "/conclusion/", 
            "text": "Conclusion", 
            "title": "5) Conclusion"
        }, 
        {
            "location": "/conclusion/#conclusion", 
            "text": "", 
            "title": "Conclusion"
        }
    ]
}